---
title: Final Prediction
author: Vivian Nguyen
date: '2022-11-07'
slug: []
categories: []
tags: []
---

This is blog post #8 (the final one!) in a series of analytical posts in lieu of the 2022 midterms. This post is affiliated with Gov 1347: Election Analytics, a course at Harvard University in the department of Government.

---
```{r setup, include = FALSE, message = FALSE}
# # Set up
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(readr)
library(stargazer)
library(usmap)
library(rmapshaper)
library(sf)
library(insight)
library(scales)

# 
# expert_ratings <- read_csv("expert_rating.csv")
# historical_results <- read_csv("historical_results.csv")
# df <- read_csv("econ_mid_genball_results.csv")
# d2022 <- read_csv("house_cands.csv")
# cook <- read_csv("cook.csv")
# inside <- read_csv("inside_elections.csv")

nat_model_data <- read_csv("nat_model_data.csv")
```

# Overview
Welcome to the final election forecasting update of this election cycle! It's been a long journey, but alas, we are one day out from Election Day. In today's blog post, I am going to present my final predictions in detail, which will involve model formulas, model descriptions, model coefficients, model interpretations, model validation, prediction confidence intervals, and visualizations. 

# National Model
Starting in [week 2](https://vivian-1372.github.io/Election-Analytics/post/2022-09-19-local-and-national-economy/), I wanted to build out a good national model with a mix of fundamental variables to predict the national two-party vote shares of the parties. I ultimately chose to stick with fundamentals because in [weeks 5](https://vivian-1372.github.io/Election-Analytics/post/2022-10-10-the-air-war/) and [6](https://vivian-1372.github.io/Election-Analytics/post/2022-10-19-the-ground-game/), I found that campaign activity, whether it be on air or on the ground, didn't significantly or directly impact electoral outcomes. In [week 7](https://vivian-1372.github.io/Election-Analytics/post/2022-10-24-shocks/), I reached the same conclusion regarding shocks. 

The best iteration of my model is the most recent one, which includes only 4 predictors: absolute GDP growth (Quarter 8, the last of the election cycle), the average generic ballot support of the House incumbent party (weighted by recency of the poll), an indicator for whether or not the House incumbent party is also the president's party during a midterm year, and the average congressional approval rate. Absolute GDP growth is the economic variable I chose to capture how voters reward and punish the incumbent House party, and it came about after multiple considerations of other economic variables like unemployment rate, GDP growth percentage, and RDI change percentage. Average generic ballot support of the party in control of the House reflects how much the nation wants the currently-leading party to retain that congressional power. The binary indicator for `President's-Party-House-Incumbent-Midterm` is my attempt to capture the well-known fact that the president's party consistently performs poorly in midterm elections. The indicator is equal to 1 if the party in control of the House happens to be the president's party in a midterm year, and to 0 otherwise. Lastly, the average congressional approval rate was included because I believe it gives us a sense of how satisfied voters are with Congress, generally. This is distinct from the generic ballot support metric, with the former telling us how much voters like Congress's performance, which (perhaps unintuitively) doesn't necessarily match 1-to-1 with how much voters want the incumbent party in power.

```{r congressional approval vs. generic ballot, include = FALSE}
plot(nat_model_data$year[17:37], nat_model_data$cong_approv[17:37], type = 'l', main = "Congressional Approval and Generic Ballot Support over the Years", xlab = "Year", ylab = "Percentage", col = "orange")
lines(nat_model_data$year, nat_model_data$pp_generic_ballot, col = "purple")
legend(1979, 23, legend = c("Congressional Approval", "Generic Ballot"), col = c("orange", "purple"), lty = 1:1)
```
```{r echo = FALSE, fig.cap = 'Disconnect between Congressional Approval and Incumbent Generic Ballot.'}
knitr::include_graphics("CA_GB.png")
```
As shown above, generic ballot support for the incumbent party in the House doesn't move with general approval of Congress! This was surprising to me at first, but was a good sign that I wasn't introducing too much collinearity into my model by including both measures.   

## Model Details

### Vote Share Formula
I wanted to use this model predict both national incumbent two-party vote share and national incumbent seat count, so I came up with two final regression formulas that incorporated the aforementioned fundamental predictors.  
```{r echo = FALSE, fig.cap = 'National Democratic Two-Party Vote Share Regression Formula.'}
knitr::include_graphics("nat_VS_formula.png")
```

### Vote Share Regression
In the modeling the incumbent party's two-party vote, which is the Democrat's vote share this election, we get some interesting coefficients. See below for this model's coefficient values, variable significance, and the model's overall evaluation metrics (like R-squared and adjusted R-squared).

```{r echo = FALSE, fig.cap = 'National Democratic Two-Party Vote Share Regression Output.'}
knitr::include_graphics("nat_VS_reg.png")
```
The small, but negative, coefficient attached to absolute GDP is rather unintuitive, but my hypothesis is that for GDP, voters respond to levels and relative change differently. Previous weeks' work has shown that positive percentage change is associated with higher incumbent vote share, as we'd expect, but positive absolute change is negatively correlated with incumbent party vote share perhaps because voters slightly punish incumbents when they deem the absolute change is not "large enough." For every additional billion dollars of GDP change between the 7th and 8th quarters, the incumbent party loses 0.014 percentage points. 

The generic ballot support variable has a coefficient of 0.472, which makes sense - if voters generally support the incumbent party in polls, the party can expect to do well. For every percentage point higher the generic ballot support is for the incumbent party in polls, the incumbent party earns an additional 0.472 percentage points of vote share. 

The only variable with a larger coefficient than generic ballot support is the president's-party-incumbent-midterm variable. Midterm years in which the incumbent party is the president's party see the incumbent party losing 2.788 percentage points of vote share. This is consistent with the decades of poor president's party performance we've observed. 

Lastly, with every 1 percentage point increase in congressional approval, the incumbent party gains one 0.092 percentage points in vote share. Again, this makes sense, as more voter satisfaction with the current Congress should be correlated with higher electoral support for the House incumbent party to remain in power. 

I would like to quickly note my model's final adjusted R-squared, 0.872, which is a nice improvement from [week 2's](https://vivian-1372.github.io/Election-Analytics/post/2022-09-19-local-and-national-economy/) 0.292. 

### Vote Share Validation
```{r vote share validation, include = FALSE}
nat_model_data <- nat_model_data %>%
  filter(year >= 1980)
outsamp_errors <- sapply(1:1000, function(i){
  years_outsamp <- sample(nat_model_data$year, 8)
  outsamp_mod <- lm(H_incumbent_party_majorvote_pct ~ GDP_growth_qt + 
               pp_generic_ballot + midterm_pres_party_inc_party + cong_approv, nat_model_data[!(nat_model_data$year %in% years_outsamp),])
  outsamp_pred <- predict(outsamp_mod, newdata = nat_model_data[nat_model_data$year %in% years_outsamp,])
  outsamp_true <- nat_model_data$H_incumbent_party_majorvote_pct[nat_model_data$year %in% years_outsamp]
  mean(outsamp_pred - outsamp_true)
})
 
hist(outsamp_errors, breaks = 20,
     xlab = "",
     main = "Mean Out-of-Sample Residual for\n Vote Share Model (1000 runs of CV)")
```
```{r vote share validation prinout, echo = FALSE, fig.cap = 'National Democratic Two-Party Vote Share Regression Output.'}
knitr::include_graphics("nat_VS_val.png")
```
Above is the histogram of the mean out-of-sample residuals over 1,000 runs of cross-validation for my model. The distribution looks roughly normally distributed, with a mean a little below 0, tiny tails, and most residuals within [-2, 2].  

### Seat Count Formula
Next, I ran the same model again, but this time with incumbent party seat count as the response variable. Below are the regression formula and regression output for this model.
```{r echo = FALSE, fig.cap = 'National Democratic Party Seat Count Regression Formula.'}
knitr::include_graphics("nat_S_formula.png")
```

### Seat Count Regression
```{r echo = FALSE, fig.cap = 'National Democratic Seat Count Regression Output.'}
knitr::include_graphics("nat_S_reg.png")
```
The model coefficients of the seat count model have the same signs as those of the vote share model, but different magnitudes because of the switch from predicting vote share, which can only be in the interval [0, 100], to predicting seat count, which can be [0, 435]. It is really the sign of the coefficients that matter for interpretation, and since those haven't changed, I will only briefly comment on this regression output. 

It's helpful to see the relationship between the president's-party-incumbent-midterm effect and *seat count* because now, it becomes clear that when the president's party is trying to defend its House incumbency in midterm years, like this year, it faces a steep uphill battle - when the indicator variable is 1, the incumbent party is predicted to lose around 29 seats. 

### Seat Count Validation
```{r seat count validation, include = FALSE}
nat_model_data <- nat_model_data %>%
  filter(year >= 1980)
outsamp_errors <- sapply(1:1000, function(i){
  years_outsamp <- sample(nat_model_data$year, 8)
  outsamp_mod <- lm(seats ~ GDP_growth_qt + 
               pp_generic_ballot + midterm_pres_party_inc_party + cong_approv, nat_model_data[!(nat_model_data$year %in% years_outsamp),])
  outsamp_pred <- predict(outsamp_mod, newdata = nat_model_data[nat_model_data$year %in% years_outsamp,])
  outsamp_true <- nat_model_data$seats[nat_model_data$year %in% years_outsamp]
  mean(outsamp_pred - outsamp_true)
})
 
hist(outsamp_errors, breaks = 20,
     xlab = "",
     main = "Mean Out-of-Sample Residual for\n Seat Count Model (1000 runs of CV)")
```
```{r seat count validation prinout, echo = FALSE, fig.cap = 'National Democratic Two-Party Vote Share Regression Output.'}
knitr::include_graphics("nat_S_val.png")
```
Above is the histogram of the mean out-of-sample residuals over 1,000 runs of cross-validation for my model again, this time for prediction of incumbent seat count. The distribution looks roughly normally distributed, with a mean at around 0 and most residuals within [-20, 20]. Neither our vote share nor our seat count model seem to perform poorly when tested with out-of-sample data, so we may move forward with prediction now!  

## National Predictions

!!! MENTION WHERE DATA TO MAKE PREDICTION IS FROM
Using the two models above, I predict that the House incumbent (Democratic Party) two-party vote share will be **48.56%** (47.36, 49.83) and the set count will be **209** (198, 221). 
!!! add visualizations

# District Models

## Model Details

## District Predictions

```{r}
# 
# historical_results %>%
#   group_by(year) %>%
#   summarize(count = n())
# 
# expert_ratings %>%
#   group_by(year) %>%
#   summarize(count = n()) 
```

```{r}
# expert_ratings$district <- as.numeric(expert_ratings$district)
# df$district_num <- as.numeric(df$district_num)
# df <- df %>%
#   rename(district = district_num)
# 
# district_data <- expert_ratings %>% 
#   filter(year != 2022) 
# 
# newdf <- left_join(df, expert_ratings, by = c("year", "state", "district"))
# newdf <- drop_na(newdf)
# 
# sds = unique(historical_results$district_id)
# models <- list()
# models2 <- list()
# r2 <- c()
# r22 <- c()
# preds <- c()
# for (sd in sds){
#   temp <- historical_results %>%
#     filter(district_id == sd)
#   
#   if (nrow(temp) > 0)
#   {
#     m <- glm(demWin ~ incumbent, data = temp, family = binomial(link = logit))
#     models[[length(models)+1]] <- m
#   
#     r2 <- c(r2, mcf_r_squared = map_dbl(m, ~with(summary(m), 1 - deviance/null.deviance)))
#   }
#   #Make a 2022 prediction and append to list
#   # preds <- c(preds, predict(m2, 2022data))
# }
# 
# summary(glm(demWin))
# mean(r2, na.rm = TRUE)
# hist(r2)
# median(r2)
```


```{r}
# expert_ratings$district <- as.numeric(expert_ratings$district)
# historical_results$district <- as.numeric(df$district_num)
# df <- df %>%
#   rename(district = district_num)
# 
# district_data <- expert_ratings %>% 
#   filter(year != 2022) 
# 
# newdf <- left_join(historical_results, expert_ratings, by = c("year", "state", "district"))
# newdf <- drop_na(newdf)
# 
# sds = unique(historical_results$district_id)
# models <- list()
# models2 <- list()
# r2 <- c()
# r22 <- c()
# preds <- c()
# for (sd in sds){
#   temp <- newdf %>%
#     filter(district_id == sd)
#   
#   if (nrow(temp) > 0)
#   {
#     m <- lm(dem_votes_major_percent ~ incumbent + avg_rating, data = temp)
#     models[[length(models)+1]] <- m
#   
#     r2 <- c(r2, summary(m)$r.squared)
#   }
#   #Make a 2022 prediction and append to list
#   # preds <- c(preds, predict(m2, 2022data))
# }
# 
# mean(r2, na.rm = TRUE)
```

```{r YAO district models for competitive ones, include = FALSE}
# Selecting columns
# 
# 
# avg_ratings <- expert_ratings %>% 
#   select(year, state, district, avg_rating)
# 
# historical_results <- historical_results %>% 
#   mutate(incumbent = case_when(DemStatus == "Incumbent" ~ 1, T ~ 0), demWin = case_when(WinnerParty == "D" ~ 1, T ~ 0), open = case_when((DemStatus == "Challenger" & RepStatus == "Challenger") ~ 1, T ~ 0))
#   
# 
# hist_data <- historical_results %>% 
#   filter(year != 2022) %>% 
#   # left join as there aren't ratings for every district
#   group_by(state, district) %>% 
#   filter(n() > 1) %>% # Filtering out single data rows
#   group_nest() %>% 
#   mutate(data = map(data, ~unnest(., cols = c())))
# 
# hist_test_data <- house_cands_22 %>% 
#   group_by(state, district) %>% 
#   group_nest() %>% 
#   mutate(data = map(data, ~unnest(., cols = c())))
# 
# models <- hist_data %>% 
#   mutate(model = map(data, ~lm(dem_vote_major_percent ~ incumbent, 
#                                   data = .x))) %>% 
#   select(-data)
# 
# model_results <- models %>% 
#   mutate(RS = map_dbl(model, ~summary(.x)$r.squared), ARS = map_dbl(model, ~summary(.x)$adj.r.squared))
# 
# pred_2022 <- hist_test_data %>%
#   # inner join as there may not be historical models for some districts
#   inner_join(models, by = c("state", "district")) %>% 
#   mutate(pred = map_dbl(.x = model, .y = data, ~predict(object = .x, newdata = as.data.frame(.y)))) %>%
#   select(state, district, pred)
# 
# hist(model_results$RS)
# mean(model_results$RS, na.rm = TRUE)
# mean(model_results$ARS, na.rm = TRUE)
# 
# #^^ those should be near 1, no???? 
# 
# testt <- historical_results %>%
#   mutate(wrong = case_when((demWin == 1 & incumbent == 1) ~ 1, T ~ 0), win = case_when((demWin == 1 & incumbent == 0) ~ 1, T ~ 0), rw = case_when((demWin == 0 & incumbent == 1) ~ 1, T ~ 0)) %>%
#   summarize(count = n(wrong))
# 
# sum(testt$wrong)
# sum(testt$win)
# sum(testt$rw)
# sum(testt$incumbent)
# 
# testt %>%
#   group_by(demWin, incumbent, open) %>%
#   summarize(count =n())

#--------------
# Joining the data and nesting by state and district
# train_data <- avg_ratings %>% 
#   filter(year != 2022) %>% 
#   # left join as there aren't ratings for every district
#   left_join(historical_results, by = c("year", "state", "district"))  %>%
#   group_by(state, district) %>% 
#   filter(n() > 1) %>% # Filtering out single data rows
#   group_nest() %>% 
#   mutate(data = map(data, ~unnest(., cols = c())))
# 
# d2022$district <- as.numeric(d2022$district )
# d2022 <- d2022 %>%
#   filter(cand_party == "Democratic") %>%
#   distinct()
# 
# test_data <- avg_ratings %>% 
#   left_join(d2022, by = c("state", "district")) %>%
#   filter(year == 2022) %>% 
#   group_by(state, district) %>% 
#   group_nest() %>% 
#   mutate(data = map(data, ~unnest(., cols = c())))
# 
# # Building TERRIBLE models
# models <- train_data %>% 
#   mutate(model = map(data, ~lm(dem_votes_major_percent ~ avg_rating + incumbent, 
#                                   data = .x))) %>% 
#   select(-data)
# 
# # Extracting TERRIBLE model results
# model_results <- models %>% 
#   mutate(RS = map_dbl(model, ~summary(.x)$r.squared), ARS = map_dbl(model, ~summary(.x)$adj.r.squared))
# 
# # Predicting 2022 with a TERRIBLE model
# pred_2022 <- test_data %>%
#   # inner join as there may not be historical models for some districts
#   inner_join(models, by = c("state", "district")) %>% 
#   mutate(pred = map(.x = model, .y = data, ~predict(object = .x, newdata = as.data.frame(.y), interval = 'confidence'))) %>%
#   select(state, district, pred)
# 
# hist(model_results$RS)
# mean(model_results$RS, na.rm = TRUE)
# mean(model_results$ARS, na.rm = TRUE)
# length(model_results)
# 
# pred_2022 %>%
#   unnest_wider(col = pred) %>%
#   distinct() %>%
#   mutate(winner = case_when(...1 >= 50 ~ "Democrat", T ~ "Republican")) %>%
#   select(state, district, ...1, ...3, ...5, winner) %>%
#   rename("prediction" = ...1, "lower" = ...3, "upper" = ...5)
# 
# d2022
```


```{r district models for non-competitive ones; just use incumbent?, include = FALSE}

## use expert ratings and incumbency for all 435, see if it agrees with my all expert preds + incumb model for the competitive ones. 
# 
# cook
# cook %>%
#   group_by(year) %>%
#   summarize(count = n())
# 
# 
# 
# newdf <- left_join(historical_results, expert_ratings, by = c("year", "state", "district"))
# newdf <- drop_na(newdf)
# 
# sds = unique(historical_results$district_id)
# models <- list()
# models2 <- list()
# r2 <- c()
# r22 <- c()
# preds <- c()
# for (sd in sds){
#   temp <- newdf %>%
#     filter(district_id == sd)
#   
#   if (nrow(temp) > 0)
#   {
#     m <- lm(dem_votes_major_percent ~ incumbent, data = temp)
#     models[[length(models)+1]] <- m
#   
#     r2 <- c(r2, summary(m)$r.squared)
#   }
#   #Make a 2022 prediction and append to list
#   # preds <- c(preds, predict(m2, 2022data))
# }
# 
# hist(r2)
# mean(r2, na.rm = TRUE)

```

```{r district models with ALL districts + expert preds - turnout, include = FALSE}
# expert_ratings$district <- as.numeric(expert_ratings$district)
# # df$district_num <- as.numeric(df$district_num)
# # df <- df %>%
# #   rename(district = district_num)
# 
# district_data <- expert_ratings %>% 
#   filter(year != 2022) 
# 
# newdf <- left_join(df, expert_ratings, by = c("year", "state", "district"))
# 
# newdff <- df %>% 
#     full_join(expert_ratings %>% filter(year < 2022 & year > 2010), by = c("year", "state", "district")) #%>%
#     #mutate_if(is.numeric, function(x) replace_na(x, 3.5))
# 
# sum(is.na(newdff$avg_rating))
# 
# newdff$avg_rating[is.na(newdff$avg_rating)] <- mean(newdff$avg_rating, na.rm = TRUE)
# 
# newdf <- drop_na(newdf)
# 
# sds = unique(house_cands_22$st_cd_fips)
# models <- list()
# r2 <- c()
# preds <- c()
# for (sd in sds){
#   # Filter for dataframe with just district sd
#   temp <- newdff %>%
#     filter(st_cd_fips == sd)
#   
#   # Fit linear model for dem two-party vote-share
#   if (nrow(temp) > 0)
#   {
#     m <- lm(DemVotesMajorPercent ~ average_support + incumb + avg_rating, data = temp)
#     models[[length(models)+1]] <- m
#   
#   # Find r^2 and add to list of r^squareds
#     r2 <- c(r2, summary(m)$adj.r.squared)
#   }
# 
#     
#   
#   # Make a 2022 prediction and append to list
#   # new_data <- turnout22 %>%
#   #   filter(st_cd_fips == sd) %>%
#   #   rename(turnout = historical_turnout_by_district)
#   # preds <- c(preds, predict(m, new_data))
# }
# 
# hist(r2, breaks = 50)
# mean(r2, na.rm = TRUE)

```


```{r}
#national model... expert prediction? 
# need data for this
# nat_model_updated_seats <- lm(seats ~ GDP_growth_pct + DSPIC_change_pct + GDP_growth_qt + 
#                pp_generic_ballot + midterm_pres_party_inc_party + expert_prediction, data = nat_model_data)

```
## New Forecast
Include confidence intervals for predictions !! 

---
**References**

[1] 
